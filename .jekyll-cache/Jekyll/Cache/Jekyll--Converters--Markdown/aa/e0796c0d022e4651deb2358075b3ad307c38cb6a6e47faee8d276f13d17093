I"À#<h1 id="research">Research</h1>

<p><img src="http://localhost:4000/images/respic/signal_data_AI.png" alt="" style="width: 300px; float: left; margin: 0px 2px" />
Our overarching goal is to broaden the understanding of sensory interactions from a science and engineering perspective, by analyzing the associated signals (and data). We utilize critical thinking, systematic experimentation, and in depth analysis, enabling us address questions we find most interesting.
<!-- This direction of research is possible thanks also to Neeraj's broad background with different research themes: speech signal modeling and audio signal processing (at IISc), understanding speech perception using behavioral and neural signals (at CMU), sound-based respiratory health diagnosis (at IISc), and spatial audio cognition (at Fraunhofer IIS). --></p>

<p>Here are some themes that we currently work on:</p>

<p><strong>Sensory sensing from a signal processing perspective</strong><br />
We intend to understand how different sensory systems perform the task of mechano-electro-transduction, and computationally model it in ways which can provide alternative methods for sampling and representation of continuous-time signals. One topic which we have carved out from this interest is â€œinformation-richâ€™â€™ sampling of non-stationary signals. Here, the goal is to <em>sample not for</em> reconstruction of the complete signal <em>but sample only for</em> reconstruction of the information of interest. Our initial work in this direction has proposed methods such as signal dependent sampling <a href="https://ieeexplore.ieee.org/abstract/document/6288659">Ref 1</a>, <a href="https://ieeexplore.ieee.org/abstract/document/6983916">Ref 2</a>, and extrema-triggered sampling of time-varying signals for instantaneous frequency estimation <a href="https://www.sciencedirect.com/science/article/pii/S0165168415001383">Ref 3</a>.<br />
<em>Keywords: time-series analysis, information theory, sparse signal recovery, compressive sensing, time-frequency analysis, physical and neural processes in sensory systems, signal representations</em></p>

<p><strong>Analyzing conversations and build conversational machines</strong><br />
Humans use social interactions in a manner enabling sharing of thoughts, learning and creating knowledge. This has helped to understand the world we live in and also engineer it. Spoken communication is a key form of social interaction. Often we spend our day immersed in listening to or speaking in a conversation. Some conversations are quick and some are confusing, some start gradually and some end abruptly. Some conversations have stayed in our memory for quite some time. Can the variants of spoken conversation be classified by analyzing the sensory signals exchanged during a conversation? Through this work we aim to explore human spoken conversations by capturing and analyzing the vocal, facial, breathing, heart beats and pupil signals of the participating individuals. This is a long-term effort involving testing of several hypothesis and solving multiple sub-problems.<br />
<em>Keywords: real-time multi-channel data acquisition and synchronization, signal processing, machine learning and NNs, psychology-based experiment design, perception and human cognition</em></p>

<p><strong>Signal (and data) analysis for human healthcare</strong><br />
You might recall an incident in which a doctor listened to your body sounds such as lungs or heart using a stethoscope.
The art of listening to body sounds is referred to as <em>auscultation</em> and dates back to the 1820s. Our goal is to further the understanding of body sounds by analyzing them using signal processing and machine learning, and evaluate the possibility of using them for remote, cost-effective, scalable disease screening and diagnosis. This requires creating a database of the sound signals associated with organs of interest, and then exploring discovery of patterns associated with specific disease, and evaluating the explainability of the results. Pursuing one work along this direction, we (with LEAP Lab) attempted analysis of respiratory sounds such as breathing, cough and speech collected during the pandemic from individuals with and without COVID-19 <a href="http://eprints.iisc.ac.in/67641/1/coswara-2020-4811-4815.pdf">Ref1</a>,<a href="https://www.sciencedirect.com/science/article/pii/S0885230821001157">Ref2</a>.<br />
<em>Keywords: time-series analysis, signal processing, audio signal processing, machine learning and NNs, inferential statistics, full stack web development, biosciences</em></p>

<p><strong>Signal (and data) analysis for climate sciences</strong><br />
When you go into the woods, the liveliness of the forest can be perceived by listening to the sounds. The cacophony of sounds draws contributions from many different species, natural phenomena such as wind, rain, and thunder, and man-made vehicles and machines. We intend to focus on the ecology sounds to demonstrate ways to monitor forestâ€™s response to changing weather and climate conditions, allowing prediction of adversities. This is a long-term effort involving testing of several hypothesis and solving multiple sub-problems.<br />
<em>Keywords: wildlife sensor design and deployment, data analysis, signal processing and machine learning, ecological sciences, full stack web development</em></p>

<p><strong>Exploratory data analysis and visualization</strong><br />
Data science being an interdisciplinary field, brings challenges and opportunities for designing creative ways to visualize data. Sometimes a good visualization can highlight hidden patterns in data and light up new ideas for solving a problem. We recognize this talent and try pulling data from various sources to illustrate some interesting patterns.<br />
<em>Keywords: visualization, sonification, animating data science and AI concepts, linear algebra, full stack web development</em></p>

<p><strong>Audio-driven brain computer interfacing</strong><br />
In 2014-2021, several EEG data decoding studies have demonstrated correlation between EEG signal and the attended audio signal in a listening experiment. A recent study by us (with International Audio Labs) also corroborated these findings <a href="https://www.cl.uzh.ch/dam/jcr:e4b2bbe9-2648-4224-8a18-439ba0ad0ebd/bookVoiceID_final.pdf#page=23">Ref 1</a>. We are looking forward to scale up this effort and evaluate the feasibility for audio-driven brain computer interfacing.<br />
<em>Keywords: EEG signal capture and analysis, signal processing, machine learning, audio processing</em></p>

<p><strong>Behavioral experiments to understand cognition and build AI</strong><br />
Unlike neuroimaging experiments like EEG and fMRI, behavioral experiments are designed to capture simple behavioral responses such as button press, reaction time, etc., to specific tasks in a listening or visual experiment. Interestingly, these simple responses when carefully captured can be analyzed to understand human cognition. We have explored analyzing aspects such as talker perception <a href="https://asa.scitation.org/doi/full/10.1121/1.5084044">Ref1</a>, impact of language on talker perception <a href="https://asa.scitation.org/doi/full/10.1121/10.0002462">Ref2</a>, and currently ongoing, spatial cognition. We look forward to continue learning from this direction of work and translate our findings to make AI algorithms.<br />
<em>Keywords: psychology and cognitive sciences, vision and audition, data science and inferential statistics, AI</em></p>

<!-- ![](http://localhost:4000/images/respic/SmartTip.png){: style="width: 250px; float: left; margin: 0px  10px"}
One of the  projects back from my job-proposal is to develop nanofabricated STM tips. The idea behind these â€œsmart tipsâ€ is to use the technologies that were developed over decades in nanofabrication and make them available for scanning probe by using a nano-device instead of the traditional STM tungsten tip. One gains the flexibility of using different functionalities that are known from the fields of nanofabrication and mesoscopic physics. We are collaborating with the group Simon Groeblacher at TU Delft to realize this concept, benefitting from their unparalleled micro/nano fabrication know how.  A prototype of a smart tip is shown to the left. See publications in Microsyst Nanoeng, Nanotechnology, and PRB. -->

<!-- **Ultra-stable SI-STM instrument.**  ![](http://localhost:4000/images/respic/STMHead.png){: style="width: 250px; float: right; margin: 0px 10px"}
For SI-STM, having the most stable STM head is key. We have used finite element simulations, good choices in material science, and craftsmanship to build the most stable STM head in the world, to our knowledge. See publication in RSI. -->

<!-- ![](http://localhost:4000/images/respic/SciPost.png){: style="width: 70%; float: center; margin: 0px"} -->

<h3 id="-and-more-as-we-continue-to-learn-and-do">â€¦ and more as we continue to learn and do.</h3>

<p><img src="http://localhost:4000/images/respic/ai_mind.png" alt="" style="width: 250px; float: center; margin: 0px 10px" /></p>
:ET